\documentclass{article}

\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{lscape}
\usepackage{mathptmx}
\usepackage[scaled=.90]{helvet}
\usepackage{courier}
%\usepackage{lipsum}% just to generate filler text
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{longtable}

\begin{document}


\section{CSD for High density MEAs}

The concepts presented here require first obtaining the CSD of the recorded LFP. Any method would present the same utility. Thanks to the density and scales of the BioCAM 4096, in our case a ``classical'' numerical derivation of the CSD is the most adecuate route to take. A numerical Laplacian Operator which reduces the cross effect of rectangular grids is the convolution of the data with the following matrix \cite{Lindberg90}:
\begin{equation}
\nabla^2_{1/3}=(2/3)
\begin{pmatrix}
  0 & 1 & 0 \\
  1 & -4 & 1 \\
  0 & 1 & 0
\end{pmatrix}
+1/3
\begin{pmatrix}
  0.5 & 0 & 0.5 \\
  0 & -2 & 0 \\
  0.5 & 0 & 0.5
\end{pmatrix}  
\end{equation}
This sort of operators are extremely sensitive to edges. The noise of the data can be perceived in the spatial domain as rapidly varying edges. A Gaussian spatial denoising can be performed before or after the Laplacian operator to the data in order to reduce excessive borders. Our Gaussian blur filter has a $\sigma$ value of three electrodes or $126 \mu m$. Intuitively this means that it smooths over each structure smaller than the soma of a typical pyramidal cell. 
This is  in accordance to our mean field approach, where we expect smooth differentiable fields over the scale of the neurons, but not in finer scales (for details see \cite{Bedard11}, further work on this subject will be presented elsewhere \cite{IsabelYo}).

On previous works it has been argued that numerical difference operators sacrifice all data on the borders of the array. For electrode arrays of lesser density, this could posse a problem, but in our case we only renounce to 27 out of 4095 channels (one is grounded). More sophisticated methods such as iCSDA \cite{Leski2011} and kCSDA \cite{Potworowski2011} prove little advantage in estimating the sources under the ohmic, isotropic and homogeneous assumptions at the scales involved in our data, and turn out to be computer-costly for high density MEAs and dense temporal sampling.  



\section{CSD and disjoint components}

One of the most obvious uses of the CSD in place of the LFP has been overlooked. This might have occurred because  samplings at lesser spatial density did not permit extending the implications of the concept of density. Density is \emph{quantifiable entity per unit of space}, or, in other words, concentration of something distributed in space. In this case, the ``something'' is the sources and sinks of electric current, which can be identified with the interchange of charged ions from the inside to the outside of cellular membranes. As these are indicators of activity in the neurons, the CSD is also, indirectly, a \emph{``density of local activity''}. The advantages of this representation of the activity in place of the LFP have been stated many times (better spatial resolution, mostly). To the best of our knowledge, the conceptual advantages of using this density as such has never been done. 

On the other hand, Center of Mass Analysis (CMA) has been used previously in order to track putative  ``trajectories'' of the activity or information across neural tissue or even the whole brain \cite{Chao05, Chao07, Manjarrez07, Manjarrez09}. In such works the chosen ``mass'' was an heuristic measure adecuate for representing the corse-grain picture of displacement of activity. These present the usual problem of justifing the choice made on practical grounds. For calculating a vectorial average, the adecuate wheight per vector is a density, a positive quantity that measures the amount of something per unit volume. CSD sepparates naturally the measured activity into two distinct sets, namely, the set of all sources and the set of all sinks, separated by the zero set. The sepparation is not quantitative but qualitative: even when the signs of electric charge are arbitrary, the distintion between them is not. Therefore, CSD represents on each of the two sets (sinks and sources) exactly what we need, a unit of concentration per unit space, in this case, per area.  Also, these separation by the zero value set of the CSD is in contrast to LFP or EEG measurements, where the zero set has no precise meaning as it only represents a practical ground value. This is aknowledged in most of the literature, where only the scales of the LFP recording are shown, without reference to the sign of the values.  

A first visual inspection of the CSD colormap at a given instant in time shows that both sources and sinks appear in rather large and significant patches. These appear to correspond to the expected double and triple poles of fyring neurons in structured tissue \cite{} and inspired us to separate the CSD subsets into their \emph{disjoint components}. This concept is used in mathematics to denote subsets of a given geometric set that do not touch each other. Their rough correspondence to poles of firing neurons would be most useful in tracking the successive locus of the activity.


To clarify the concept and how we are we using it an example is in order. In figure \ref{ejemplodisjuntos} we show geometrical sets enclosed by simple curves as color patches, where every color indicates a Set. The set A consists of a single component, that means that one can make  path between any two points in the set that consist entirely of points of the same set. In contrast, the set B is made of two disjoint components, labeled B1 and B2. If one chooses a point in each of the components, one cannot draw a path joining them without leaving the set. But each one of those components is connected, meaning that if we choose the pair of points in one component, then again we can trace a path joining them in the set. So the set B consists of two disjoint components, but each component can be regarded as a connected set on their own. Likewise the set C is made of three disjoint components. 
 
Theoretically, CSD would divide the set of measuring points into three subsets: the set of all sources, the set of all sinks, and the border between them i.e, the set of all measuring points that have a CSD positive value, the set of those who have a negative CSD value, and the set of those points having exactly the zero value in CSD space.
Due to the finite precision of the measurement, and of the validity of the mean field approach the sinks and sources set can make contact with each other. This is because our measurements produce a function over a discrete set, a continuous function corresponding CSD is only a theoretical tool. 
Even more, noise between patches corresponding to a single set could make a ``bridges'' between components, making identification of poles inexact. In order to avoid this we create a ``thick zero'' set of all the recording points whose CSD values are inside a small error interval around zero. This separates even more the components of the set, represents our ignorance of precise borders and helps us to simplify the following analysis. 
 
 \section{CSD and Center of Mass}
 
 Densities provide the adequate conceptual tool to obtain ``Centers of Mass'' or vector averages. A density is a measure of a quantity per spatial spread, at precise locus in space. CSD has, in contast to mass density, negative values. But these signs are a matter of convention, it is more important to say that it has two different cualitative propiertes and that sepparates the domain into two sets. Each set has then a total ``mass'', the integral of the absolute value of the CSD. Then the concept of ``Center of Mass'' applies naturally. Once that we sepparate the two sets and apply the concept in each one independently, even the sign convention of the densities seases to be a problem, as it cancels in the formula for the CM:
 \begin{equation}
 \langle x(t) \rangle =\frac{\sum_j x_{j} (t) I_{j} (x,t)} {\sum_j I_{j}(x,t)}
 \end{equation}
 An instantaneous Center of Mass, thus calculated, would present little or none physiological interpretation. The activity could be spread over non-convex sets (which would include disjoint and concave sets) and so the vector average would be located in points where there is no measurement of activity. In our example the use of the CA complex of the hippocampus proves an ideal example. This structure is concave, and during epileptic burst, can have activity spread over many disjoint sites simultaneously. If our method is reliable, then, even under such constrains we may be able to track the activity and provide some physiological interpretation. That is why we have used data that was obtained from an experiment in which 4AP (bla bla for Franco).
 
 We must not take out of sight the phenomena which is producing the CSD distribution: neurons have a finite size, and the effect of their action a finite extent. Every patch that we can delimit as a disjoint component of the Sources/Sink set indicates a coherent, joint active group of units. A center of mass for each disjoint component would then indicate an average locus for this active group, a putative center of activity to which we can give a position, intensity and time. An instantaneous snapshot of such information would depict the centers of activity of an ordered structure of neurons and their relative intensities. But it is in the successive analysis of a series of such snapshots that this analysis can show its true power. The appearance , displacement, and disappearance of such centers could reveal very subtle details of the dynamics of neural activity. Very small, fast displacements of the Centers of mass, could reveal slight asynchronism between neurons which could belong to the same active group. Larger and  slower than a few firing periods displacements could indicate physiological connections between various groups.
 
 
 \section{The Recipe}
 
 In order to perform the analysis described here, we summarize the steps.
 \begin{enumerate}
 \item Acquire high density electro-physiological data. The data must be two or three dimensional in space. Distance between the recording sites must be of the order of the size of the typical neurons involved, and such sites must be on a dense enough grid to perform numerical difference operations over them. The sampling frequency should be enough to detect sub-threshold activity, of at least 5kHz. Also, the data should come from structured tissue. Unordered, highly homogeneous tissue, will not yield significant results. 
\item Obtain, frame by frame, the CSD from the data. Any method may be used, but computer expensive techniques are discouraged if the data is dense enough. Common sense and tests are guidelines here. We recommend simpler, difference based approaches. 
\item Separate the CSD data into three sets: sources, sinks, and border or indiscernible (below error) data.
\item Perform disjoint component analysis in the sources and sinks sets, frame by frame.
  A single pass algorithm could be used here \cite{Vincent91, Abubaker07}.
\item  Obtain the CM for each disjoint component.
\item For each frame and its successor, and for each CM, perform a ``most probable successor'' detection. Here we must put numerical criteria according to the data avaible, i.e. how far awat and how different in intensity should be allowed a CM to be at time $t+1$ to be considered succesor in time of another at time $t$.
  \item plot the data and try to figure what it means.
\end{enumerate}

A series of experiments done in the laboratory on rat's hippocampus provided us with the data that we shall use to illustrate the CSD and CM analysis proposed above. 

\section{Application to epileptic-like activity in the hippocampus}
Chronologically speaking, we performed first the analysis to the most complicated data (numerically and qualitatively), which where a series of epileptic burst that occurred after 15 minutes of application of 4AP to living slices of hippocampus. The burst occurred with activity spreading over many sites simultaneously. This turned out to be a test of the applicability and robustness of our method. The necessity of having good error value for the ``thick zero'' set was more apparent here than anywhere else. Also the posibilty to create new structures of numerical data was seen here.  But also the most strickig features of the propagation of activity, coded now as trajectories of the CM, could be seen here, as the epileptic burst covered the whole CA structure. 

\section{Application to evoked activity in idem.}

The evoked activity seems to yield much cleaner results, as we can now compare the trajectories of successive, very similar events. Our CSD and CM analysis can be used to detect consistent features in these experiments, such as the direction the waxing and wanning of activity. Small deviations from the trajectory in successive experiments that are above the numerical error could indicate inconsistencies in the network below. 

\section{Application to spontaneous activity in idem.}

We had one experiment in which we could trace very nicely the data on DG and CA simultaneously without adding excitatory drugs. 

\bibliographystyle{plain}
\bibliography{../Reportes/BiblioReportes01} 


\end{document}

